{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f2414f-9517-4261-adc6-a331db3ab4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing problematic properties...\n",
      "  Unset: fs.s3a.threads.keepalivetime\n",
      "  Unset: hadoop.security.groups.shell.command.timeout\n",
      "  Unset: hadoop.service.shutdown.timeout\n",
      "  Unset: yarn.resourcemanager.delegation-token-renewer.thread-timeout\n",
      "  Unset: yarn.federation.gpg.webapp.connect-timeout\n",
      "  Unset: yarn.federation.gpg.webapp.read-timeout\n",
      "  Unset: fs.s3a.retry.interval\n",
      "  Unset: fs.s3a.retry.throttle.interval\n",
      "  Unset: fs.s3a.connection.ttl\n",
      "  Unset: fs.s3a.multipart.purge.age\n",
      "  Set fs.s3a.threads.keepalivetime = 60\n",
      "  Set hadoop.security.groups.shell.command.timeout = 0\n",
      "  Set fs.s3a.retry.interval = 500\n",
      "  Set fs.s3a.retry.throttle.interval = 100\n",
      "  Set fs.s3a.connection.ttl = 300000\n",
      "\n",
      "Setting MinIO configuration...\n",
      "  Set fs.s3a.endpoint = http://minio:9000\n",
      "  Set fs.s3a.access.key = admin\n",
      "  Set fs.s3a.secret.key = password\n",
      "  Set fs.s3a.path.style.access = true\n",
      "  Set fs.s3a.impl = org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "  Set fs.s3a.connection.ssl.enabled = false\n",
      "  Set fs.s3a.connection.timeout = 60000\n",
      "  Set fs.s3a.socket.timeout = 60000\n",
      "  Set fs.s3a.connection.establish.timeout = 5000\n",
      "\n",
      "Configuration complete!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start with a clean session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO_connection\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Clear all problematic properties first\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "# List of properties that might have duration strings\n",
    "duration_props = [\n",
    "    \"fs.s3a.threads.keepalivetime\",\n",
    "    \"hadoop.security.groups.shell.command.timeout\",\n",
    "    \"hadoop.service.shutdown.timeout\",\n",
    "    \"yarn.resourcemanager.delegation-token-renewer.thread-timeout\", \n",
    "    \"yarn.federation.gpg.webapp.connect-timeout\",\n",
    "    \"yarn.federation.gpg.webapp.read-timeout\",\n",
    "    \"fs.s3a.retry.interval\",\n",
    "    \"fs.s3a.retry.throttle.interval\",\n",
    "    \"fs.s3a.connection.ttl\",\n",
    "    \"fs.s3a.multipart.purge.age\"\n",
    "]\n",
    "\n",
    "print(\"Clearing problematic properties...\")\n",
    "for prop in duration_props:\n",
    "    hadoop_conf.unset(prop)\n",
    "    print(f\"  Unset: {prop}\")\n",
    "\n",
    "# Now set them with numeric values where needed\n",
    "numeric_props = {\n",
    "    \"fs.s3a.threads.keepalivetime\": \"60\",  # seconds as number\n",
    "    \"hadoop.security.groups.shell.command.timeout\": \"0\",  # 0 seconds\n",
    "    \"fs.s3a.retry.interval\": \"500\",  # milliseconds\n",
    "    \"fs.s3a.retry.throttle.interval\": \"100\",  # milliseconds\n",
    "    \"fs.s3a.connection.ttl\": \"300000\",  # 5 minutes in ms\n",
    "}\n",
    "hadoop_conf.set(\n",
    "    \"fs.s3a.aws.credentials.provider\",\n",
    "    \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n",
    ")\n",
    "for prop, value in numeric_props.items():\n",
    "    hadoop_conf.set(prop, value)\n",
    "    print(f\"  Set {prop} = {value}\")\n",
    "\n",
    "# Configure MinIO\n",
    "minio_configs = {\n",
    "    \"fs.s3a.endpoint\": \"http://minio:9000\",\n",
    "    \"fs.s3a.access.key\": \"admin\",\n",
    "    \"fs.s3a.secret.key\": \"password\", \n",
    "    \"fs.s3a.path.style.access\": \"true\",\n",
    "    \"fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "    \"fs.s3a.connection.timeout\": \"60000\",\n",
    "    \"fs.s3a.socket.timeout\": \"60000\",\n",
    "    \"fs.s3a.connection.establish.timeout\": \"5000\",\n",
    "}\n",
    "\n",
    "print(\"\\nSetting MinIO configuration...\")\n",
    "for key, value in minio_configs.items():\n",
    "    hadoop_conf.set(key, value)\n",
    "    print(f\"  Set {key} = {value}\")\n",
    "\n",
    "print(\"\\nConfiguration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f53901a-084f-4b4c-bbc0-6f2fa86b558e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Current Hadoop Properties ===\n",
      "fs.s3a.select.output.csv.record.delimiter: \\n\n",
      "yarn.app.mapreduce.am.job.committer.cancel-timeout: 60000\n",
      "fs.s3a.select.input.csv.quote.character: \"\n",
      "fs.s3a.path.style.access: true\n",
      "fs.s3a.access.key: admin\n",
      "fs.s3a.select.input.compression: none\n",
      "fs.s3a.max.total.tasks: 32\n",
      "fs.s3a.vectored.read.min.seek.size: 128K\n",
      "fs.s3a.select.output.csv.quote.fields: always\n",
      "fs.s3a.vectored.read.max.merged.size: 2M\n",
      "fs.s3a.socket.timeout: 60000\n",
      "ha.failover-controller.new-active.rpc-timeout.ms: 60000\n",
      "fs.s3a.select.input.csv.header: none\n",
      "mapreduce.outputcommitter.factory.scheme.s3a: org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory\n",
      "ha.health-monitor.rpc-timeout.ms: 45000\n",
      "fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "adl.http.timeout: -1\n",
      "hadoop.security.kms.client.timeout: 60\n",
      "fs.s3a.select.enabled: true\n",
      "yarn.nodemanager.health-checker.timeout-ms: 1200000\n",
      "ipc.client.connect.timeout: 20000\n",
      "fs.s3a.committer.staging.tmp.path: tmp/staging\n",
      "ha.zookeeper.session-timeout.ms: 10000\n",
      "hadoop.registry.zk.session.timeout.ms: 60000\n",
      "ha.failover-controller.graceful-fence.rpc-timeout.ms: 5000\n",
      "mapreduce.reduce.shuffle.read.timeout: 180000\n",
      "fs.s3a.executor.capacity: 16\n",
      "yarn.dispatcher.drain-events.timeout: 300000\n",
      "fs.s3a.multiobjectdelete.enable: true\n",
      "mapreduce.shuffle.connection-keep-alive.timeout: 5\n",
      "mapreduce.reduce.shuffle.connect.timeout: 180000\n",
      "yarn.timeline-service.client.drain-entities.timeout.ms: 2000\n",
      "hadoop.registry.zk.connection.timeout.ms: 15000\n",
      "fs.ftp.timeout: 0\n",
      "fs.s3a.multipart.purge: false\n",
      "fs.s3a.connection.establish.timeout: 5000\n",
      "yarn.resourcemanager.proxy.timeout.enabled: true\n",
      "fs.s3a.downgrade.syncable.exceptions: true\n",
      "yarn.nodemanager.runtime.linux.runc.hdfs-manifest-to-resources-plugin.stat-cache-timeout-interval-secs: 360\n",
      "yarn.nodemanager.elastic-memory-control.timeout-sec: 5\n",
      "fs.s3a.audit.enabled: true\n",
      "fs.s3a.select.input.csv.record.delimiter: \\n\n",
      "fs.s3a.accesspoint.required: false\n",
      "fs.s3a.retry.limit: 7\n",
      "ipc.client.rpc-timeout.ms: 120000\n",
      "fs.s3a.committer.name: file\n",
      "fs.s3a.threads.max: 96\n",
      "fs.s3a.threads.keepalivetime: 60\n",
      "hadoop.security.groups.shell.command.timeout: 0\n",
      "ha.failover-controller.cli-check.rpc-timeout.ms: 20000\n",
      "fs.s3a.select.input.csv.quote.escape.character: \\\\\n",
      "fs.s3a.multipart.threshold: 128M\n",
      "fs.s3a.ssl.channel.mode: default_jsse\n",
      "fs.s3a.change.detection.mode: server\n",
      "fs.s3a.buffer.dir: ${env.LOCAL_DIRS:-${hadoop.tmp.dir}}/s3a\n",
      "dfs.ha.fencing.ssh.connect-timeout: 30000\n",
      "fs.s3a.fast.upload.active.blocks: 4\n",
      "yarn.client.failover-retries-on-socket-timeouts: 0\n",
      "yarn.nodemanager.node-labels.provider.fetch-timeout-ms: 1200000\n",
      "fs.s3a.block.size: 32M\n",
      "yarn.nodemanager.node-attributes.provider.fetch-timeout-ms: 1200000\n",
      "fs.s3a.change.detection.version.required: true\n",
      "fs.s3a.select.output.csv.field.delimiter: ,\n",
      "fs.s3a.committer.staging.conflict-mode: append\n",
      "yarn.resourcemanager.application-timeouts.monitor.interval-ms: 3000\n",
      "fs.s3a.select.output.csv.quote.escape.character: \\\\\n",
      "mapreduce.reduce.shuffle.fetch.retry.timeout-ms: 30000\n",
      "fs.viewfs.overload.scheme.target.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "fs.s3a.list.version: 2\n",
      "fs.s3a.connection.maximum: 500\n",
      "fs.s3a.paging.maximum: 5000\n",
      "fs.s3a.readahead.range: 64K\n",
      "fs.s3a.attempts.maximum: 5\n",
      "fs.s3a.select.input.csv.comment.marker: #\n",
      "hadoop.security.group.mapping.ldap.connection.timeout.ms: 60000\n",
      "fs.s3a.etag.checksum.enabled: false\n",
      "fs.s3a.retry.interval: 500\n",
      "mapreduce.task.exit.timeout.check-interval-ms: 20000\n",
      "hadoop.http.idle_timeout.ms: 60000\n",
      "yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms: 1000\n",
      "fs.s3a.select.errors.include.sql: false\n",
      "hadoop.zk.timeout-ms: 10000\n",
      "fs.s3a.multipart.size: 64M\n",
      "fs.s3a.select.input.csv.field.delimiter: ,\n",
      "fs.s3a.socket.recv.buffer: 8192\n",
      "fs.s3a.connection.ssl.enabled: false\n",
      "yarn.resourcemanager.proxy.connection.timeout: 60000\n",
      "hadoop.security.group.mapping.ldap.directory.search.timeout: 10000\n",
      "fs.s3a.committer.threads: 8\n",
      "yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts: 3\n",
      "yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs: 3600\n",
      "fs.s3a.secret.key: password\n",
      "ipc.client.connect.max.retries.on.timeouts: 45\n",
      "fs.s3a.fast.upload.buffer: disk\n",
      "fs.s3a.select.output.csv.quote.character: \"\n",
      "mapreduce.task.stuck.timeout-ms: 600000\n",
      "fs.s3a.committer.staging.unique-filenames: true\n",
      "yarn.nodemanager.container-executor.exit-code-file.timeout-ms: 2000\n",
      "fs.s3a.retry.throttle.interval: 100\n",
      "yarn.app.mapreduce.am.hard-kill-timeout-ms: 10000\n",
      "yarn.resourcemanager.node-removal-untracked.timeout-ms: 60000\n",
      "fs.s3a.connection.ttl: 300000\n",
      "fs.s3a.aws.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\n",
      "fs.s3a.assumed.role.session.duration: 30m\n",
      "mapreduce.task.exit.timeout: 60000\n",
      "fs.s3a.assumed.role.credentials.provider: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\n",
      "fs.s3a.committer.abort.pending.uploads: true\n",
      "fs.s3a.committer.magic.enabled: true\n",
      "mapreduce.task.timeout: 600000\n",
      "fs.s3a.connection.timeout: 60000\n",
      "fs.s3a.change.detection.source: etag\n",
      "fs.AbstractFileSystem.s3a.impl: org.apache.hadoop.fs.s3a.S3A\n",
      "yarn.client.application-client-protocol.poll-timeout-ms: -1\n",
      "fs.s3a.socket.send.buffer: 8192\n",
      "hadoop.security.group.mapping.ldap.read.timeout.ms: 60000\n",
      "fs.s3a.retry.throttle.limit: 20\n",
      "fs.s3a.endpoint: http://minio:9000\n"
     ]
    }
   ],
   "source": [
    "# Print ALL Hadoop properties\n",
    "print(\"=== Current Hadoop Properties ===\")\n",
    "iterator = hadoop_conf.iterator()\n",
    "while iterator.hasNext():\n",
    "    prop = iterator.next()\n",
    "    key = prop.getKey()\n",
    "    value = prop.getValue()\n",
    "    if \"timeout\" in key.lower() or \"s3a\" in key.lower():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d42cc6e-f709-4638-bed4-78be8e44c68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to MinIO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/05 17:49:37 WARN Base64: JAXB is unavailable. Will fallback to SDK implementation which may be less performant.If you are using Java 9+, you will need to include javax.xml.bind:jaxb-api as a dependency.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Write successful!\n",
      "Reading from MinIO...\n",
      "+----------+---------+\n",
      "|   Company|Valuation|\n",
      "+----------+---------+\n",
      "|Databricks|     3000|\n",
      "|    Nvidia|     1000|\n",
      "|    OpenAI|     2000|\n",
      "+----------+---------+\n",
      "\n",
      "✓ Read successful!\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "data = [(\"Nvidia\", 1000), (\"OpenAI\", 2000), (\"Databricks\", 3000)]\n",
    "df = spark.createDataFrame(data, [\"Company\", \"Valuation\"])\n",
    "\n",
    "# Test write\n",
    "try:\n",
    "    print(\"Writing to MinIO...\")\n",
    "    df.write.mode(\"overwrite\").parquet(\"s3a://raw-data/test.parquet\")\n",
    "    print(\"✓ Write successful!\")\n",
    "    \n",
    "    # Test read\n",
    "    print(\"Reading from MinIO...\")\n",
    "    df_read = spark.read.parquet(\"s3a://raw-data/test.parquet\")\n",
    "    df_read.show()\n",
    "    print(\"✓ Read successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8aeb2bf-e905-4244-b7c2-529ebb39f2c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|  Company|StockPrice|\n",
      "+---------+----------+\n",
      "|    Apple|       100|\n",
      "|   Google|       200|\n",
      "|Microsoft|       300|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7395de7-f4e3-4969-960c-bfd347cd9ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting credentials provider...\n",
      "  Set fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting credentials provider...\")\n",
    "hadoop_conf.set(\n",
    "    \"fs.s3a.aws.credentials.provider\",\n",
    "    \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n",
    ")\n",
    "print(\"  Set fs.s3a.aws.credentials.provider = org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd7d1e-f14b-4615-9de7-a6ff8e82fa04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
